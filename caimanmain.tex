\section{Introduction}
\label{sec:intro}

GPUs are hard to program I guess.

\subsection{Separation of Concerns}
\label{subsec:separation}

Our running example throughout this paper will be used to illustrate the performance trade--offs inherent in conditional logic and device synchronization.  Specifically, we will be building the \code{select_sum} function, which takes in three arrays `v1`, `v2`, and `v3`, and returns the sum of either `v2` or `v3` depending on the sign of `v1`.

A naive \texttt{Rust} solution for this might look something like the following (for some fixed size of array):
%
\begin{lstlisting}

select_sum(v1 : [i64; N], v2 : [i64; N], v3 : [i64; N]) -> i64 {
  if sum(v1) < 0 {
    sum(v2)
  } 
  else {
    sum(v3)
  }
}
\end{lstlisting}
%
This code is fairly straightforward to understand, and has the significant upside of having its behavior exactly dependent on the abstraction of using an existing (working) function.
This approach, however, is not necessarily performant.

While we can often rely on an optimizing compiler to work out a reasonably fast arrangement of this code, the compiler cannot always assume the \textit{intent} of the programmer, restricting potential optimizations.
As a result, the programmer may need to rewrite this code to change performance, but without changing the semantic meaning of this code:
%
\begin{lstlisting}

select_sum(v1 : [i64; N], v2 : [i64; N], v3 : [i64; N]) -> i64 {
  int sum2 = sum(v2);
  int sum3 = sum(v3);
  if (sum(v1) < 0) {
    return sum2;
  } 
  else {
    return sum3;
  }
}
\end{lstlisting}
%
In this fairly simple case, we likely expect the compiler to optimize these programs to be about the same.  We may quickly become less confident when dealing with more complicated logic, however, often resulting in a need to fight the compiler to perform exactly the operations the programmer intends.  This is especially true when we introduce another device into the equation, and the substantial increase in decisions an implementation makes about when to send information between devices and when to start asynchronous operations.

To examine this specific issue in our example, suppose \code{v2} and \code{v3} are large arrays.  Rather than fiddling with the order of operations in \code{select_sum}, we might hope to improve performance by moving the \code{sum} operations on these large arrays to the GPU, with code we hope looks like the following:
%
\begin{lstlisting}
select_sum(v1 : [i64; N], v2 : [i64; N], v3 : [i64; N]) -> i64 {
  if (sum(v1) < 0) {
    sum_gpu(v2)
  }
  else {
    sum_gpu(v3)
  }
}
\end{lstlisting}
%

\subsection{Combinatoric Explosion}
\label{subsec:combinatoric}

\section{Background}
\label{sec:background}

\subsection{Submitting to the GPU}
\label{subsec:submission}

\subsection{Promises}
\label{subsec:promise}

\paragraph{CUDA, SYCL}

\paragraph{Halide, Scheduling}

\label{subsec:linear}

\section{Practical Caiman}
\label{sec:practical}

To explain how Caiman works, we will work through the example shown in the introduction, namely \code{select_sum}.  For this illustration, we will be using code written the user-facing Caiman frontend, as opposed to the Caiman IR described in more detail in~\ref{sec:engineering}.

A Caiman program consists of a series of functions written across two languages: the \textit{specification} language and the \textit{implementation} language.  Informally, a function written in the specification language describes the semantic behavior of the program, while a function written in the implementation language describes how that program is implemented on the host machine.  This is the core mechanism by which Caiman separates semantic and performance concerns.

Additionally, a Caiman specification must deal with one of three distinct properties of the operation it is specifying: the value calculations, the timeline of the events and synchronizations, and manipulations of existing memory.  We give each of these ``kinds" of specification functions a unique name, respectively they are the \textit{value}, \textit{timeline}, and \textit{spatial} specifications.  Any implementation must implement one of each kind of specification, though specifications may be used by multiple implementation functions.

We will start by showing how to implement \code{select_sum} in Caiman's specification language(s), before moving onto describing the choice of implementations Caiman provides.  We will also examine some intuition of why the Caiman typechecker is able to validate an implementation against a given specification, though the formal model and proof will be deferred to section~\ref{sec:formal}.

\subsection{Value Specification}
\label{subsec:value}

Caiman specifications are written as functions with a Rust-like header and declarative bodies.  We emphasize declarative here to mean that declaration order does not matter (as we will see, we are essentially building a dependency graph).  The complete syntax can be summarized as follows:
%
\begin{lstlisting}

// function header, any number of arguments
spec_kind name(arg : type) -> return_type {
  var :- expression
  returns var
}
\end{lstlisting}
%
The specific expressions allowed for each variable declaration depend on the kind of specification.  All specifications, however, share expressions for function calls and ternary conditional expression:
%
\begin{lstlisting}

// function call
fn_name(expr)
// the usual notion of conditions
if cond then expr1 else expr2
\end{lstlisting}
%
The most intuitive Caiman specification to start with is often the \textit{value} function, which describes what calculations are needed for each data value in the program.  We will take a deep dive into describing the value specification for \code{select_sum} before returning to definitions for Caiman's other specification languages in section~\ref{subsec:spec}.  

Our value specification for \code{select_sum} is as follows:
%
\begin{lstlisting}

val select_sum(v1: [i64; N], v2: [i64; N], v3: [i64; N]) -> i64 {
    sum1 :- sum(v1)
    sum2 :- sum(v2)
    sum3 :- sum(v3)
    condition :- sum1 < 0
    result :- if condition then sum2 else sum3
    returns result
}
\end{lstlisting}
%
We have written this specification to be more verbose than needed for the sake of providing a more detailed examination of the semantics being used here.  Many of these lines can be condensed or combined (we can just write \code{if sum(v1) < 0 ...}, for example).

Nevertheless, this declaration more-or-less mirrors our naive C code, notably replacing conditional \code{if/else} blocks with the ternary expression \code{if-then-else}.  This distinction is more than just syntactic, Caiman's specification language is designed to have no internal control flow, as hinted at by the syntax.

Additionally, since Caiman's operations are unordered (we can freely move or combine each declaration here without changing the specification meaning), we do not have any requirement that these operations must be executed in the order written.

More precisely, this specification gives us constraints on what values this function must produce, but leaves the details up to the actual implementation.  We can exactly enumerate these requirements as follows:
%
\begin{itemize}
\item \code{v1}, \code{v2}, and \code{v3} are all arrays of type \code{i64}, and this function produces data of type \code{i64} (this is the usual type constraint placed by a function header).
\item The specification variables \code{sum1}, \code{sum2}, and \code{sum3} depend on \code{v1}, \code{v2}, and \code{v3}, respectively.  For each of these, we must apply \code{sum} to produce our respective value.
\item \code{condition} depends on having calculated \code{sum1} already \textit{at some point}, and also have the constant \code{0} as a value.
\item Similarly, \code{result} depends on having calculated \code{condition}, \code{sum1}, and \code{sum2}, thus requiring the calculations of all of their dependencies to have been done.
\item Finally, \code{returns result} informs us that this function will return the \code{result} value.
\end{itemize}
%
Importantly, however, this specification provides formal requirements; as we will shortly in subsection~\ref{subsec:implementation}, Caiman implementations of this specification which do not produce the specified values will fail to typecheck and will be rejected at compile time.  Indeed, we can safely say that each specification variable becomes a type we can refer to while typechecking an implementation function.  We must first take a short diversion into examining Caiman function definitions.

\subsubsection{Function Equivalence}
\label{subsec:equivalence}

We need to take a short dive into how the \code{sum} function is being used in this \code{select_sum} specification.  In Caiman, every function used in a specification must be a \textit{function equivalence class}.  Function equivalence classes consist of a name associated with one or more function definitions (which may be defined in Caiman or externally) that the programmer considers equivalent.

The most immediate use for function classes can be seen with the following definition of \code{sum} that we include in the \code{select_sum} file:
%
\begin{lstlisting}

feq sum {
    extern(cpu) pure sum_cpu([i64; N]) -> i64
    extern(gpu) pure sum_gpu([i64; N]) -> i64
}
\end{lstlisting}
%
Here we are simply saying that we consider the \code{cpu} and \code{gpu} definitions of \code{sum} to be equivalent.  The key reason for introducing these equivalence classes is to allow for multiple implementations of the same function to coexist in the same program, and to allow the user to fearlessly call any particular version in their implementation of some specification.  

Note that \code{select_sum} must also be in such a function equivalence class -- syntactically we name this equivalence class to be the same as our defined value function, in this case just \code{select_sum}.  In this way, Caiman specification functions can call other specification functions without introducing assumptions about the implementations of those functions.  

Importantly, however, Caiman does not attempt to prove this assertion or require any more annotation than exactly the type of the function provided here (the external implementations of \code{sum} could be buggy, and Caiman makes no claim about preventing this).  Similarly, we also make no attempt to verify that an external function implementing a Caiman value specification will match the semantics of that specification, and leave this work to the user.  Stylistically, this means that Caiman's guarantees and utility are strongest when a majority of the code being used in a Caiman program is written in Caiman rather than made external.  

\subsection{Implementation Language}
\label{subsec:implementation}

With our value specification and function equivalence classes in hand, we can now implement a Caiman program for \code{select_sum}.  As a reminder, all the code we have written so far provides typing information for our actual implementation here, and is not compiled into an executable program without this implementation.

Frontend Caiman implementations more-or-less resemble Rust code, with the addition of specifying which particular specification(s) they are implementing.  As noted above, we will only focus on implementing our value specification for now.  Our first implementation of \code{select_sum} is thus as follows:
%
\begin{lstlisting}

fn select_sum_impl(v1: [i64; N], v2: [i64; N], v3: [i64; N]) 
  -> i64 impls select_sum,... {
    if sum_cpu(v1) < 0 { 
        sum_cpu(v2)
    }
    else {
        sum_cpu(v3)
    }
}
\end{lstlisting}
%
This is a valid Caiman implementation of this program, meant to show that in many cases, the programmer can essentially implement code similar to standard languages, where information can mostly be inferred.  For understanding the typechecking work Caiman does on this program, however, it is perhaps more informative to show the explicit Caiman type annotations we can provide for such a program.  When we hand-write these annotations, we produce the following (equivalent) program:
%
\begin{lstlisting}

fn select_sum_impl(
  v1: [i64; N] @ val.v1,
  v2: [i64; N] @ val.v2,
  v3: [i64; N] @ val.v3)
  -> i64 @ node(val.result)
  impls select_sum,... {
    let s1 : i64 @ node(val.sum1) = sum_cpu(v1);
    let cond : bool @ node(val.condition) = s1 < 0;
    let res : @ node(val.result) = if cond {
        let s2 : i64 @ node(val.sum2) = sum_cpu(v2);
        s2
    }
    else {
        let s3 : i64 @ node(val.sum3) = sum_cpu(v3);
        s3
    };
    res
}
\end{lstlisting}
%
Our focus in this rewrite is to expose the (baseline) types used by a Caiman implementation.  Each variable in an implementation has 2 components: an raw datatype and three specification types (we are only showing the value specification type for now for simplicity).  \code{s1}, for example, has a raw datatype of \code{i64}, and a (value) specification type of \code{node(val.sum1)}.

The part of the type \code{node(val.sum1)} has three pieces, \code{node}, \code{val}, and \code{sum1}, with the following meanings:
%
\begin{itemize}
\item \code{node} states that the given type is a node in some specification function (as opposed to an \code{input} or an \code{output} to that specification)
\item \code{val} states that the given type is a part of the value specification this function is implementing, \code{select_sum} in this case
\item \code{sum1} states that we are working with the specific node named \code{sum1}.
\end{itemize}
%
Crucially, this is part of the type of the implementation variable as much as the datatype \code{i64}, though the specification type can be erased when compiling Caiman code.  In other words, if we instead wrote the (incorrect) annotation:
%
\begin{lstlisting}

let s1 : i64 @ node(val.sum2) = sum(v1);
\end{lstlisting}
%
Then our code would fail to compile.  Interestingly, this line alone would be enough to fail to compile, as we defined \code{val.sum2=sum(v2)}, and since our implementation variable \code{v1} has type \code{input(val.v1)}, the types of \code{v2} and \code{v1} don't ``match up".

An important observation is the type of \code{res}, which is derived from the \code{if-else} condition logic being applied.  In essence, we consider control flow in Caiman's implementation language to have a return type, which can only be a \code{unit} if the associated specification type is a \code{unit}.  We can show that this particular conditional logic works out, because the true case returns a result associated with the value type \code{v2}, while the false case returns a result associated with the value type \code{v3}.  If we swapped the logic to instead produce \code{v3} in the true case and \code{v2} in the false case, we would have a type error.

With our types in hand, we can now rewrite our original code to reorder the sum operations to before the conditional logic, as we originally desired.  The (type-inferred) code in Caiman can be simply written as follows:
%
\begin{lstlisting}

fn select_sum_impl_2(v1, v2, v3) 
  -> i64 impls select_sum,... {
    let s2 = sum_cpu(v2);
    let s3 = sum_cpu(v2);
    if sum_cpu(v1) < 0 {
        s2
    }
    else {
        s3
    }
}
\end{lstlisting}
%
A visibly typed version of this code can be found in the appendix.  This implementation still typechecks and so maintains the value semantics of the \code{select_sum} specification -- a proof of this claim will be given in~\ref{sec:formal}.  Importantly, by providing this function definition, we now have two implementations in the \code{select_sum} function equivalence class -- these can be called elsewhere in Caiman by name with \code{select_sum_impl} or \code{select_sum_impl_2}.

We can now address the notion of actually using function equivalence in our implementation.  Unfortunately, we are not able to yet write our call to \code{sum_gpu} (as alluded to in~\ref{subsec:equivalence}), since we will first need to specify more details about synchronizing to another device (the GPU), and we have so far assumed that the CPU is our local host and so does not need synchronization.

To avoid introducing unnecessary complications to our straightforward equivalence relation, we will instead introduce another implementation of sum on the CPU, extending our equivalence class of \code{sum}:
%
\begin{lstlisting}

feq sum {
    extern(cpu) pure sum_cpu_1([i64; N]) -> i64
    extern(cpu) pure sum_cpu_2([i64; N]) -> i64
    extern(gpu) pure sum_gpu([i64; N]) -> i64
}
\end{lstlisting}
%
This example is clearly synthetic and meant to be illustrative, but even in this case we could imagine a second definition of \code{sum_cpu}: specialized to be multi-threaded while our first definition is single-threaded.

We can freely use either definition within our \code{select_sum} function; for example, we could now write the following code, and the types of each value are identical to what we had before:
%
\begin{lstlisting}

fn select_sum_impl_3(v1: [i64; N], v2: [i64; N], v3: [i64; N]) 
  -> i64 impls select_sum,... {
    if sum_cpu_2(v1) < 0 { 
        sum_cpu_1(v2)
    }
    else {
        sum_cpu_2(v3)
    }
}
\end{lstlisting}
%
This mechanism to freely interchange function definitions is core to Caiman's goal of separating performance decisions and specification, as discussed in Section~\ref{sec:intro}.  Note that here we have introduced yet another definition to the \code{select_sum} function equivalence class to explore swapping out specific definitions.  In this way, we can now avoid the usual combinatorial explosion of logic being checked by relying on the value specification to maintain static consistency.

It is important now to address the limitations of Caiman's function equivalence classes, since they are designed to be very simple (and we hope transparent).  Equivalence classes only apply to exactly function calls, and must either be filled by external functions (which are not checked by Caiman), or by implementations in Caiman.

This means that basic properties like arithmetic or logical equivalence are not reasoned about by Caiman unless explicitly declared.  Concretely, for example, we consider \code{1+1} to be a distinct value from the constant \code{2}.  We will examine potential extensions to Caiman's minimal equivalence system when we discuss future work in Section~\ref{subsec:future}.

\subsection{Timeline and Spatial Specifications}
\label{subsec:spec}

We have thus far focused on a vertical slice through Caiman with the value specification language.  We can now take the intuition and ideas from this explanation to work through Caiman's other two specification languages.  

It is worth noting up--front that Caiman's particular choice of specification languages is not rooted in any sort of experimentation, but instead an intuition of what is needed for the examples we care about -- you could extend the ideas presented here to write an entirely different set of specification languages.  Detailing this more abstract notion of what a specification language is, however, is out of scope of this write--up, and will be left to another, more theoretical paper.

We capture the intent of the value specification language in Caiman as reasoning about the data produced and used by our computation.  When we are working with another device (such as a GPU), however, it is also important to be precise about what threading and memory resources we interact with.  More specifically, we would like to be able to break apart synchronization and memory resources across control flow, in much the same way as the value language allows us to (safely) reuse data and decompose computation on that data into carefully designed pieces.

To achieve this goal, we also introduce the \emph{timeline} and \emph{spatial} specification languages to Caiman.  These languages follow the syntax and declarative approach used by the value specification, but with operations intended to capture the intent of synchronization and memory primitives used when scheduling GPU operations.

A Caiman synchronization mirrors that of the GLSL and WGPU submission processes, described in Section~\ref{subsec:submission}.  This process consists of exactly 3 events, described as a host managing the devices A and B, where device A is providing the data to run and device B is providing the computation and result:
%
\begin{enumerate}
\item The host requests an \emph{encoding} location from device B, which is then given to device A.
\item After device A has finished encoding data, the host \emph{submit}s that device B can begin computation.
\item Once device B has finished the computation, the host allows device A to \emph{synchronize} and access the written data.
\end{enumerate}
%
Caiman employs classic promise semantics for this model, described in~\ref{subsec:promise}.  For the timeline specification, then, we introduce an \code{Event} type, which informally describes a point in time.  Note that we implicitly also introduce subtypes for each step of this process to keep track of the logical flow, where each requirement is maintained structurally as being in dependency order. We also introduce the following three operations to match the stages described:
%
\begin{lstlisting}

// Given an Event, starts an encoding on device B,
//   also produces a 'local' event associated with device A
//   along with a 'remote' event associated with device B
local, remote :- encode_event(e)
// Given a 'remote' event, begins a submission on device B
//   produces a 'submission' event
sub :- submission_event(remote)
// Given a 'submission' and 'local event, 
//  produces a 'synchronization' event on device A
snc :- synchronization_event(local, sub)
\end{lstlisting}
%
We will explore a more detailed example of using the timeline language shortly in Section~\ref{subsec:fullexample}.  First, however, we will summarize the (relatively simple) spatial specification language.

The spatial language is used primarily to specify memory behaviors to help with guarantees related to implemented loops or recursion.  To this end, the spatial language provides a \code{BufferSpace} type, which defines a cluster of memory, along with exactly one operation, used to divide up this buffer space:
%
\begin{lstlisting}

// splits up a given buffer space into n>=1 evenly sized pieces:
separate_buffer_space(buff, n);
// for example, we can split a buffer in half:
buff1, buff2 :- separate_buffer_space(buff, 2);
\end{lstlisting}
%
We will not being working through any examples that define a non-trivial spatial specification, but a recursive example can be found in Appendix \xxx{TODO}.

Having defined our specification functions, we can now fully state that an implementation in Caiman must be associated with exactly one of each kind of specification.  A given implementation need not implement the entire specification of each, so long as the input and output types of that implementation are correct as stated.  Note that this means that a call into a particular Caiman implementation of a function equivalence class may result in a series of calls (some of which may be also used by other Caiman implementations of that specification).

Further, each specification could be implemented by any number of Caiman implementation functions.  Additionally, each specification function has its own equivalence class, which refers to any implementation of that specification (it is rare to use a function equivalence class other than a value specification or for external functions).

Note that a specification can always be trivial (simply taking in a single input and returning it), and so we can syntactically elide a specification in both an implementation header and a variable type.  In these cases, we assume that the type of the implementation variable associated with the missing specification function refers to the trivial specification.
\subsection{Select Sum on the GPU}
\label{subsec:fullexample}

With our timeline and spatial specification functions more carefully defined, we can now construct an implementation of \code{select_sum} that calls into the GPU to calculate either \code{sum(v2)} or \code{sum(v3)} (there are many other variations we can write, but we will start with this approach).  In Rust-like psuedocode, what we are looking to implement resembles the following logic:
%
\begin{lstlisting}
if sum_cpu(v1) < 0 {
    sum_gpu(v2)
} else {
    sum_gpu(v2)
}
\end{lstlisting}
%
First, we reiterate our value specification for ease of readability:
%
\begin{lstlisting}
val select_sum(v1: [i64; N], v2: [i64; N], v3: [i64; N]) -> i64 {
    sum1 :- sum(v1)
    sum2 :- sum(v2)
    sum3 :- sum(v3)
    condition :- sum1 < 0
    result :- if condition then sum2 else sum3
    returns result
}
\end{lstlisting}
%
Second, we provide a timeline specification for a \emph{single} synchronization (since we only call the gpu once in each branch, we need only to synchronize once).  As a result, our specification will simply lay out our four stages in sequence:
%
\begin{lstlisting}
tmln single_sync(e : Event) -> Event {
    local, remote :- encoding_event(e)
    sub :- submission_event(remote)
    sync :- synchronization_event(local)
    returns sync
}
\end{lstlisting}
%
Third, we provide a (trivial) spatial specification:
%
\begin{lstlisting}
sptl trivial_spatial(b : BufferSpace) -> BufferSpace {
    returns b;
}
\end{lstlisting}
%
Now we can provide an exact implementation for this entire function.  We will write this implementation without explicit value types, though an explicitly typed version can be found in Appendix \xxx{TODO}:
%
\begin{lstlisting}

fn select_sum_impl_gpu(v1: [i64; N], v2: [i64; N], v3: [i64; N]) 
  -> i64 impls select_sum,single_sync,trivial_spatial {
    if sum_cpu(v1) < 0 {
        let enc = encode-begin @ node(tmln.(local, remote)) { v2 } gpu;
        // copy the data from v2 into a gpu variable `v2_gpu`
        encode enc.copy[v2_gpu <- v2];
        // schedule the gpu to write the result 
        //   of sum_gpu(v2) to a variable named s2_gpu
        encode enc.call[s2_gpu <- sum_gpu(v2) @ val.sum2];
        
        // submit the calculation to the gpu
        let fence = submit @ tmln.sub enc
        
        // await the result
        let gpu_data = await @ tmln.sync
        
        // return s2_gpu from the bundled result
        gpu_data.s2_gpu
    }
    else {
        // we apply similar logic here
        let enc = encode-begin @ node(tmln.(local, remote)) { v3 } gpu;
        encode enc.copy[v3_gpu <- v3];
        
        encode enc.call[s3_gpu <- sum_gpu(v3) @ val.sum3];
        let fence = submit @ tmln.sub enc
        let gpu_data = await @ tmln.sync
        
        gpu_data.s3_gpu
        sum_cpu(v3)
    }
}
\end{lstlisting}
%
This implementation more-or-less mirrors the first approach we examined, namely calculating the sum of \code{v2} and \code{v3} inside of the condition.  Now that we have the logic for using the GPU, this approach of first calculating both before the condition will (hopefully) be more immediately appealing.

Interestingly, in this case, we can apply both operations sequentially on the GPU with only a single synchronization, but this requires an unsatisfying black--box solution where we write a \code{sum_2_arrays} type function.  Alternatively, we could write a new timeline specification which allows for two synchronizations, and if our goal were to interleave our encodings and synchronizations, then we would write a new specification.

However, if we are comfortable synchronizing twice, Caiman does provide a clean solution in the form of breaking up our implementation to use the timeline specification twice, without needing to modify any of our specifications:
%
\begin{lstlisting}

fn select_sum_impl_gpu_2(
  v1: [i64; N] @ val.v1,
  v2: [i64; N] @ val.v2,
  v3: [i64; N] @ val.v3) 
  -> i64 @ val.result
  impls select_sum,single_sync,trivial_spatial {
    let enc = encode-begin @ node(tmln.(local, remote)) { v2 } gpu;
    encode enc.copy[v2_gpu <- v2];
    encode enc.call[s2_gpu <- sum_gpu(v2) @ val.sum2];
    let fence = submit @ tmln.sub enc
    let gpu_data = await @ tmln.sync
    let s2 = gpu_data.s2_gpu;
    
    select_sum_impl_gpu_2_inter()
}
fn select_sum_impl_gpu_2_inter(
  v1: [i64; N] @ val.v1, 
  s2: i64 @ val.sum2, 
  v3: [i64; N] @ val.v3)
  -> i64 @ val.result
  impls select_sum,single_sync,trivial_spatial {
    // similar logic to compute s3
    select_sum_impl_gpu_2_ret(v1, s2, s3)
}
fn select_sum_impl_gpu_2_ret(
  v1: [i64; N], 
  s2: i64 @ val.sum2, 
  s3: i64 @ val.sum2) 
  -> i64 @ val.result
  impls select_sum,trivial_timeline,trivial_spatial {
    if sum_cpu(v1) < 0 {
        s2
    }
    else {
        s3
    }
}
\end{lstlisting}
%
This idea of breaking up a function to implement only part of a specification is crucial to using Caiman, as code written in this way can represent unfinished and can avoid duplicating computation.  The reason this code is able to typecheck is precisely because of our explicit annotations on the arguments and return type of each function -- otherwise, the program analysis needed often becomes intractable.  Since Caiman can prove that the type boundaries of each function resolve, we need not worry about the potential complexity introduced between function calls.

Another important observation is that we can use a similar idea to break apart encoding and synchroniziation, allowing a programmer to first encode an operation, then continue doing work on the device, and finally synchronize.  Note that Caiman's restrictions are such that we could not encode an operation and then never synchronize that operation (at least, within a Caiman program where we manipulate the timeline between pieces of control flow), since the types associated with the timeline and spatial specifications must match between each possible branch we could take.

Finally, it is worth acknowledging that this Caiman implementation has become quite complicated even for this simple operation.  These implementations are, by design, rather detailed about each operation that is needed to synchronize information with the GPU (or just any other device).  The intention behind this approach is to allow the user of Caiman to be as precise as possible with defining program implementation.  That being said, having implementations be this dense seemingly defeats the purpose of the guarantees Caiman can make -- if we require the programmer to both write careful specifications and implement those specifications to exacting detail, the effort to maintain this system can quickly get out of hand.

We will address this concern with a core piece of the Caiman design when we discuss in Section~\ref{sec:explication}.  First, however, we need to take a brief detour into examining a formal model for Caiman's type system, to provide formal backing and proof behind the core guarantee of Caiman's implementations actually being checked against the specification.

\section{Formal Model}
\label{sec:formal}

\begin{figure}
\begin{align*}
	\psi &\in \textbf{S}\\
	\phi &\in \textbf{C}\\
	\tau &\in \textbf{T} \\
    d &\defas d_1.d_2 \,|\,
        \tau\,\psi\,\textrm{:-}\, \phi(\psi_1) \,|\,
        \tau\,\psi\,\textrm{:-}\, \textrm{if}\, \psi_1 \,\textrm{then}\, \psi_2 \,\textrm{else}\, \psi_3 
        \\
    p &\defas d.\textrm{returns}\, \psi
\end{align*}
\caption{Hatchling Specification Syntax}
\label{fig:hatchspecsyntax}
\end{figure}

\begin{figure}
\begin{align*}
	\psi&\in \Psi\\
   	\tau&\in \textbf{T} \\
	x&\in \textbf{V}\\
	f&\in \textbf{F}\\
	c&\defas \,c_1 ; c_2 \,|\, 
        x\leftarrow f_\psi(x_1) \,|
        \\&\qquad
        x=\textrm{alloc}\,\tau \,|\,
        x\leftarrow\textrm{copy}\, x_1 \,|\, 
        x\leftarrow\textrm{ref}\, x_1 \,|
        \\&\qquad
        x\leftarrow\textrm{select}_{\psi} (x_1, x_2, x_3)\\
    p&\defas c;\textrm{return}\, x
\end{align*}
\caption{Hatchling Implementation Syntax}
\label{fig:hatchimplsyntax}
\end{figure}

\xxx[dg]{I include alloc, copy, and ref here because it feels like I need to be clear about the imperative nature of Caiman's implementation, but I actually think these end up being pretty non--interesting.  I feel like the rest of the formalism is enough in retrospect.}

In this section, we will be describing a formal model of the Caiman language. We will use this to prove our assertion that the Caiman typechecker will ensure a given (typed) implementation has the same (observational) semantics as a specification it implements.  We will also be more formally specifying these terms to precisely narrow our claim, and addressing the limitations of this presented approach.

To do this, we will start by describing a subset of the Caiman IR, a language called \textit{Hatchling}.  Hatchling has operations similar to the types and control flow of the Caiman IR, with the goal of describing exactly the type guarantees made by Caiman.

As with the Caiman IR, Hatchling has a specification and an implementation.    Our primary goal will be to setup a proof of the following theorem (although the detailed proof itself will be left to Appendix TODO):
%
\begin{theorem}
Any well-typed Hatchling implementation program with types matching those of a well-typed Hatchling specification program must, for all inputs, either fail to terminate or produce equivalent values as that specification program.
\end{theorem}
%
As an observation about this proof, the term \textit{equivalent outputs} relies on a precise notion of equivalence and values in this context.  Specifically, equivalence is defined exactly as any function that is a member of its function class (as described in subsection~\ref{subsec:equivalence}), while value refers to the series of operations that define some result.  In this sense, we observe that the operation $1+1$ is not considered to be the same value as the constant $2$, unless an equivalence is explicitly stated.

The syntax for each of these Hatchling sub-languages are defined in figures~\ref{fig:hatchspecsyntax} and~\ref{fig:hatchimplsyntax}, respectively.  These syntax rely on external sets of distinct symbols, namely $\mathbf{T}$, $\textbf{S}$, $\textbf{P}$, $\mathbf{V}$, and $\mathbf{F}$.  These sets have no explicit meaning, but intuitively represent the sets of types, specification variables, function classes, implementation variables, and function names, respectively.  Function calls in Hatchling only allow a single argument -- the extension of these proofs to multiple arguments is straightforward but mechanically irksome.  We assume that the set of types $\mathbf{T}$ includes the type $\textrm{unit}$, which we use in the typechecking semantics.  We also assume the set of specification variable symbols $\mathbf{S}$ and the set of function classes $\mathbf{C}$ includes $\mathbf{0}$, a special character that cannot be assigned to and indicates a lack of dependence.

A Hatchling specification program additionally requires 2 contexts, 1 dynamic and 1 static:
\begin{itemize}
\item 
$\Psi:\mathbf{S}\mapsto
\mathbf{T}\bigtimes
\left(\mathbf{C}\bigtimes
\left(\mathbf{S}\bigtimes\mathbf{S}\bigtimes\mathbf{S}\right)
\right)$
, which is dynamic and maps from a specification variable to both its type and its dependencies.  A variable depends either on a function or no function (semantically represented with $\top$), and has exactly 0, 1, or 3 dependencies (0 for an input, 1 for a function call, or 3 for a condition).
\item $\Phi:\mathbf{C}\mapsto\mathbf{T}\bigtimes\mathbf{T}$, which is static and maps from function classes to the input type and the output type of that function.
\end{itemize}

A Hatchling implementation program, on the other hand, similarly requires only 1 dynamic context, but requires 3 constant global contexts (where $\Delta$ and $\Psi$ are built to be derived from the specification associated with the current implementation).  These contexts are as follows:
\begin{itemize}
\item $\Gamma:\mathbf{V}\mapsto\mathbf{T}\bigtimes\left(\mathbf{S}\cup\top\right)$, the sole dynamic context, which maps from an implementation pointer to the type of the data being referred to by that pointer.  This context also includes the associated value if one has been written to the location pointed at by this variable.
\item $\Psi:\mathbf{S}\mapsto
\mathbf{T}\bigtimes
\left(\mathbf{C}\bigtimes
\left(\mathbf{S}\bigtimes\mathbf{S}\bigtimes\mathbf{S}\right)
\right)$
, which must be derived from the result of applying the typechecking rules to a well-typed Hatchling specification.  When typechecking an implementation, however, $\Psi$ is global and constant.
\item $\Phi:\mathbf{C}\mapsto\mathbf{T}\bigtimes\mathbf{T}$, which is the same as with the specification program
\item $\Xi:\mathbf{F}\mapsto\mathbf{C}$, which maps from functions to their owning function class
\end{itemize}

Finally, both Hatchling programs must specify an output type.  For the Hatchling specification program, we denote this as $\tau_\textrm{out}$, while for the Hatchling implementation program, we denote this as $\psi_\textrm{out}$.  Note that we assume that a function being a member of $\Phi$ implies that it is a well--typed function.

\subsection{Typing Semantics}

\begin{figure}
	\begin{mathpar}
   		\inferrule
   		{\Psi,\Psi' |- d_1 \qquad \Psi',\Psi'' |- d_2}
   		{\Psi,\Psi'' |- d_1.d_2}
           
   		\inferrule
   		{\Psi,\Psi' |- d_2 \qquad \Psi',\Psi'' |- d_1}
   		{\Psi,\Psi'' |- d_1.d_2}

		\inferrule
		{\psi\notin\Psi 
        \qquad \Psi(\psi_1)=(\tau_1,\_)
        \qquad \Phi(\phi)=(\tau_1,\tau_2)}
		{\Psi,
        \Psi\sqcup[\psi\mapsto\left(\tau_2,\left(\phi,\left(\psi_1,\mathbf{0},\mathbf{0}\right)\right)\right)]
         |- 
        \tau\,\psi\,\textrm{:-}\, \phi(\psi_1)}

		\inferrule
		{\psi\notin\Psi 
        \qquad \psi_1\in\Psi 
        \qquad \Psi(\psi_2)=(\tau,\_)
        \qquad \Psi(\psi_3)=(\tau,\_)}
		{\Psi,\Psi\sqcup
        [\psi\mapsto\left(\tau,
        \left(\top,
        \left(\psi_1,\psi_2,\psi_3\right)
        \right)
        \right)]
         |- 
        \textrm{if}\, \psi_1 
        \,\textrm{then}\, \psi_2 
        \,\textrm{else}\, \psi_3}
        
   		\inferrule
   		{\Psi,\Psi' |- d \qquad \Psi'(\psi)=(\tau_\textrm{out},\_)}
   		{\Psi,\Psi' |- d.\textrm{return}\,\psi}
	\end{mathpar}
	\caption{Hatchling Specification Typing Judgment}
	\label{fig:spectyping}
\end{figure}

\begin{figure}
	\begin{mathpar}
      	\inferrule
   		{\Gamma(x)=(\tau,\psi)}
        {\Gamma,\Gamma[x/(\tau,\top)]}
        
		\inferrule
		{\Gamma,\Gamma' |- c_1 \qquad \Gamma',\Gamma'' |- c_2}
        {\Gamma,\Gamma'' |- c_1;c_2}
        
        \inferrule
		{
            \Psi(\psi)=(\_,(\phi,(\psi_1,\mathbf{0},\mathbf{0}))) \qquad
            \Xi(f)=\phi \qquad
            \Gamma(x_1)=(\tau,\psi_1) \qquad
        }
        {\Gamma,\Gamma[x/(\tau,\psi)] |- x\leftarrow f_\psi(x_1)}
        
        \inferrule
		{\qquad}
        {\Gamma,\Gamma[x/(\tau,\top)] |- x=\textrm{alloc}\,\tau}
        
        \inferrule
		{\Gamma(x_1)=(\tau,\psi) \qquad \Gamma(x)=(\tau,\_)}
        {\Gamma,\Gamma[x/(\tau,\psi)] |- x=\textrm{copy}\,x_1}
        
        \inferrule
		{\Gamma(x_1)=(\tau,\psi) \qquad \Gamma(x)=(\tau,\_)}
        {\Gamma,\Gamma[x/(\tau,\psi)] |- x=\textrm{ref}\,x_1}
        
        \inferrule
        {
            \Gamma(x_i)=(\_,\psi_i) \qquad
            \Psi(\psi)=(\_,(\top,(\psi_1,\psi_2,\psi_3))) \qquad
        }
        {\Gamma,\Gamma[x/(\tau,\psi)] |- x\leftarrow\textrm{select}_{\psi} (x_1, x_2, x_3)}
        
   		\inferrule
   		{\Gamma,\Gamma' |- c, \Gamma'(x)=\psi_\textrm{out}}
        {\Gamma,\Gamma' |- c;\textrm{return}\, x}
	\end{mathpar}
	\caption{Hatchling Implementation Typing Judgment}
	\label{fig:impltyping}
\end{figure}

\xxx[dg]{TODO: If I cut the alloc and ref, then this is essentially just done I think}

\subsection{Operational Semantics}

\xxx[dg]{I don't think I actually need operational semantics, since they sort of ``fall out" from the simplified model I decided to go with}

\section{Explication}
\label{sec:explication}

As observed at the end of Section~\ref{sec:practical}, hand--writing Caiman implementations are painful and can practically lead to significant barriers to experimentation.  Specifically, the code itself is dense and can be hard to navigate, but we have found it to be anecdotally difficult to write even with compiler support.  Indeed, it feels as though the work done when implementing a Caiman program is both essentially the same work as writing a Caiman specification and can obscure the performance characteristics being explored.

The entire design of Caiman, however, is such that this task of writing an implementation from a specification can be automated with annotation.  More specifically, we can apply a form of type--directed program synthesis, as seen here: \xxx[dg]{TODO: get a few citations, this is a common enough thing}.

We use explication to describe the task of synthesizing a Caiman implementation from a Caiman specification in the presence of programmer--written implementation requirements.  To make this statement more concrete, we will start by working from example of how this looks in the Caiman frontend.

The author notes that at the time of this writing, the programs shown in this section cannot be written as--is in the Caiman frontend, due entirely to minor missing engineering work on AST transformations.  Approximately equivalent programs can be written explicitly in the Caiman IR (described in ~\ref{subsec:ir}, and the programs are included in Appendix~\xxx{TODO}), but I believe it is important to note this limitation in our actual implementation.  Consequently, more annotations may need to be included in the finished Caiman frontend (to help guide the explicator) than are shown here.

\subsection{Using Explication}

We will start by reiterating our usual value specification for \code{select_sum}:
%
\begin{lstlisting}

val select_sum(v1: [i64; N], v2: [i64; N], v3: [i64; N]) 
  -> [out : i64] {
    sum1 :- sum(v1)
    sum2 :- sum(v2)
    sum3 :- sum(v3)
    condition :- sum1 < 0
    result :- if condition then sum2 else sum3
    returns result
}
\end{lstlisting}
%
Along with a trivial timeline and spatial specification, this is enough to write a fully explicated implementation of \code{select_sum}:
%
\begin{lstlisting}

fn select_sum_expl(
  v1: [i64; N] @ val.v1,
  v2: [i64; N] @ val.v2,
  v3: [i64; N] @ val.v3
) -> i64 : val.out 
  impls select_sum,trivial_time,trivial_space{
  ???
}
\end{lstlisting}
%
With explication, we still need to provide a header (arguments and return types) and the specifications to implement.  When we write \code{???}, however, we are semantically saying that any type-safe code can be inserted to replace this \emph{multi--line hole} in the program.

The Caiman \emph{explicator} can use then types we have given this function to deduce some program that matches these types.  The explicator will not, however, be able to use any other specifications than those given, meaning that it is often the case that no such program exists.  For instance, if we had excluded \code{v1} from our arguments, the \code{select_sum} specification provides no way to construct \code{v1}, and so the explicator would fail to produce a solution.

Importantly, however, the explicator makes almost no guarantee about which program it will produce.  The only requirement we state is that, for a given explication implementation and for a given specification and (partial) implementation, the explicator will produce the same Caiman program -- in other words, the explicator must be deterministic.  

With our current implementation of Caiman, for example, the explicator makes no attempt to optimize the program being produced, and only the first valid program it happens to find is emitted.  The explication results are written to be somewhat transparent to help with identifying issues (we will show some examples of explicated programs in Appendix \xxx{TODO}), but it is theoretically preferable to avoid specific guarantees.

As a result, a key design insight for using Caiman explication is the ability to explicate a partially--implemented program.  As a concrete example, let us restrict the explicator for \code{select_sum} to ensure that the sum of \code{v2} and \code{v3} are being calculated inside of the condition rather than before:
%
\begin{lstlisting}
fn select_sum_expl_2(
  v1: [i64; N] @ val.v1,
  v2: [i64; N] @ val.v2,
  v3: [i64; N] @ val.v3
) -> i64 : val.out 
  impls select_sum,trivial_time,trivial_space{
  if ? {
    ???
  }
  else {
    ???
  }
}
\end{lstlisting}
%


\subsection{Caiman IR}
\label{subsec:ir}

\subsection{Core Algorithm}

\subsection{Controlling the Explicator}
\label{subsec:control}.

\section{Caiman Engineering}
\label{sec:engineering}

\subsection{WebGPU Target and Codegen}

\subsection{Stages of Compilation}

\section{Results}
\label{sec:results}

\subsection{Timing}

\subsection{Explication}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Future Work}
\label{subsec:future}